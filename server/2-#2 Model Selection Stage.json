{"model": "text-davinci-003", "temperature": 0, "logit_bias": {"366": 5, "1298": 5, "20662": 5, "312": 5, "41181": 5, "4895": 5}, "prompt": "#2 Model Selection Stage: Given the user request and the parsed tasks, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The assistant should focus more on the description of the model and find the model that has the most potential to solve requests and tasks. Also, prefer models with local inference endpoints for speed and stability.<im_start>user\ntell me how many zebras in the picture of /examples/c.jpg<im_end>\n<im_start>assistant\n{'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': 'public//examples/c.jpg'}}<im_end>\n<im_start>user\nPlease choose the most suitable model from [{'id': 'nlpconnect/vit-gpt2-image-captioning', 'inference endpoint': ['Salesforce/blip2-opt-2.7b', 'nlpconnect/vit-gpt2-image-captioning', 'naver-clova-ix/donut-base', 'Salesforce/blip-image-captioning-base'], 'likes': 219, 'description': '\\n\\n# nlpconnect/vit-gpt2-image-captioning\\n\\nThis is an image captioning model trained by @ydshieh in [', 'tags': ['image-to-text', 'image-captioning']}, {'id': 'Salesforce/blip-image-captioning-base', 'inference endpoint': ['Salesforce/blip2-opt-2.7b', 'nlpconnect/vit-gpt2-image-captioning', 'naver-clova-ix/donut-base', 'Salesforce/blip-image-captioning-base'], 'likes': 44, 'description': '\\n\\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Ge', 'tags': ['image-captioning']}, {'id': 'naver-clova-ix/donut-base', 'inference endpoint': ['Salesforce/blip2-opt-2.7b', 'nlpconnect/vit-gpt2-image-captioning', 'naver-clova-ix/donut-base', 'Salesforce/blip-image-captioning-base'], 'likes': 36, 'description': '\\n\\n# Donut (base-sized model, pre-trained only) \\n\\nDonut model pre-trained-only. It was introduced in ', 'tags': ['donut', 'image-to-text', 'vision']}, {'id': 'Salesforce/blip2-opt-2.7b', 'inference endpoint': ['Salesforce/blip2-opt-2.7b', 'nlpconnect/vit-gpt2-image-captioning', 'naver-clova-ix/donut-base', 'Salesforce/blip-image-captioning-base'], 'likes': 25, 'description': '\\n\\n# BLIP-2, OPT-2.7b, pre-trained only\\n\\nBLIP-2 model, leveraging [OPT-2.7b](https://huggingface.co/f', 'tags': ['vision', 'image-to-text', 'image-captioning', 'visual-question-answering']}] for the task {'task': 'image-to-text', 'id': 0, 'dep': [-1], 'args': {'image': 'public//examples/c.jpg'}}. The output must be in a strict JSON format: {\"id\": \"id\", \"reason\": \"your detail reasons for the choice\"}.<im_end>\n<im_start>assistant", "stop": ["<im_end>"], "max_tokens": 3212}